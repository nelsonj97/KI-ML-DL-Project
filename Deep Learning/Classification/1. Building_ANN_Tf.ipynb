{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f46da6",
   "metadata": {},
   "source": [
    "# Building ANN using Tensorflow 2 for Classification\n",
    "###            (Predicting whether a customer will leave the bank or not)\n",
    "Build a fully connected neural network with fully connected layer i.e with no Convolutional or Recurrent layers.\n",
    "- Hee we Will have an input vector containing different features and we'll predict an out come which will be a binary variable.\n",
    "  - because ANN can be used for both classification and regression problems.\n",
    "    - And here we're going to do it for classification problem.\n",
    "- **Dataset**:\n",
    "  - We'll be using a dataset of a Bank who collected some information about their customers.\n",
    "  - The bank observed if a customer for a certain period of time will leave or not, and they gather the outcome in the Exited \n",
    "    column.<br>\n",
    "  - The bank got all this features, to understand the correlation between the fact whether a customer will leave or not and \n",
    "    the features. \n",
    "     - i.e. The Bank want to understand the reason why customer leave the bank and once the manage to build a model to predict whether a \n",
    "       customer will leave or not, they will deploy the model on new customer. \n",
    "       - For all the customer which the model predict they will leave the bank, they will give them a special offer to those customers \n",
    "         so that they will stay in the bank.\n",
    "         - All this is called **Customer Retention**, i.e. preventing customers from leaving the bank.\n",
    "  - **Features**\n",
    "    - **The features are**:\n",
    "      - CustomerId\n",
    "      - CreditScore\n",
    "      - Geography\n",
    "      - Gender\n",
    "      - Age\n",
    "      - Tenure\n",
    "      - Balance\n",
    "      - NumOfProducts\n",
    "      - HasCrCard\n",
    "      - IsActiveMember\n",
    "  - **Exited**:\n",
    "    - Exited is a dependent variable which tells us if the customer left the bank or not.\n",
    "      - 0: Customer did not leave the bank\n",
    "      - 1: Customer left the bank\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf9319",
   "metadata": {},
   "source": [
    "# Part 0 -  libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167b06f",
   "metadata": {},
   "source": [
    "### &nbsp; 1 Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e422af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40fe5a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21.0-dev20250721\n"
     ]
    }
   ],
   "source": [
    "# Printing the tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0f0c1",
   "metadata": {},
   "source": [
    "### &nbsp; 1. Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8e9dea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "\n",
    "# Since the first, second, third column i.e. index, customerID, surname will not help in our prediction we'll drop it.\n",
    "X = dataset.iloc[:, 3 : -1].values       # we want the value of the index of all the columns except the last one\n",
    "Y = dataset.iloc[:, -1].values           # we want the value of the last column only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ac84b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the metric of features X \n",
    "  # i.e. printing all the feature, except the dependable variable\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "17e8c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Printing the dependable variable Y\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfda3a2",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Encoding categorical data\n",
    "- We've two categorical variables, i.e. Geography and Gender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceaf599",
   "metadata": {},
   "source": [
    "#### &nbsp; &nbsp; &nbsp; &nbsp; 2.1 Label Encoding the \"Gender\" column\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e. Converting the categorical variable into numerical one <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e. assigning 0 and 1 to Male and Female respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "91c329ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Le = LabelEncoder()\n",
    "\n",
    "# We've to apply the label encoding only on the gender column\n",
    "X[:, 2] = Le.fit_transform(X[:, 2])          # Meaning we want to transform all the row but only the column of index 2, ....\n",
    "                                             # ... then assigning the result to the gender column \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ecf8fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "# To Make sure that we're not more seeing the male and female in the data set\n",
    "    # Female was encoded into 0 and Male into 1\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee0ddd",
   "metadata": {},
   "source": [
    "#### &nbsp; &nbsp; &nbsp; &nbsp; 2.2 One Hot Encoding the \"Geography\" column\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e. We're performing One Hot Encoding on the Geography column because the is no **numerical order<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; relationship** or **rank** between france, germany and spain.<br> \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e. Using **numbers** would **mislead the model** into thinking some countries are **\"higher\"** or <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **\"closer\"** to others, which could hurt performance. $therefore$ One-Hot Encoding creates <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **binaries** for each country.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e.  One-Hot avoids introducing **false numerical relationships** **that** would lead to rankinking and as consequence bias the model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e94bd488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1])], remainder = 'passthrough')   # We want to apply One Hot Encoding on the Geography column\n",
    "                                                                                                        # ... i.e. index 1 of the feature variable  X\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1a56167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a4a90",
   "metadata": {},
   "source": [
    "- France was encode into [1.0, 0.0, 0.0] <br>\n",
    "- Spain was encode into [0.0, 0.0, 1.0]  <br>\n",
    "- Spain was encode into [0.0, 1.0, 0.0]  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc825f1",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 3. Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4b0c3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe80c4",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 4. Feature Scaling\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Note**: Feature Scaling is absolute compulsory in Deep Learning. <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e. We're **normalizing** or **standardizing** the data to have a mean of zero and a standard deviation of one. <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - And will be applied to all our feature variables irrespective of whether they are already in the desire scale/range <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3a0fd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Applying feature scaling feature of all the training and test set i.e. on only the feature.\n",
    "X_train = sc.fit_transform(X_train)         # fit_transform is fitted to the train set in order to avoid information leakage.\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb573444",
   "metadata": {},
   "source": [
    "# Part 2 - Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a4a65",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 1. Initializing the ANN\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Here we'll initialize the ANN as a sequence of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating a variable which is nothing than tha ANN itself\n",
    "    # i.e.ANN will be create as an object of the sequential class, which allows the building of an ANN as a sequence of layers.\n",
    "ann = tf.keras.models.Sequential()         # The Sequential class is taken from the models module of the keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cfee3",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f7d10ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The way to add layer into an ANN is to use the add() method of the dense class \n",
    "    # i.e. this class allows the creation of a fully connected layer\n",
    "ann.add(tf.keras.layers.Dense(units = 6 , activation = 'relu'))    # The Dense class is taken from the layers module of the keras library.\n",
    "                                                                   # The layer module contains tools to add layers in our ANN.\n",
    "                                                                   # units corresponds to the number of neurons we want to have in the 1st hidden layer. \n",
    "                                                    # In the input neurons will simply be our features variable starting from credit score until Customer salary.\n",
    "                                                    # The activation function is the Rectifier activation function i.e. the RELU activation function\n",
    "                                                    # The RELU activation function is one of the most popular activation functions.\n",
    "# My  unit = number of input features / 2 = 10/2 = 5\n",
    "# 5 is called the number of hidden layers or hyperparameter value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfe0bb",
   "metadata": {},
   "source": [
    "#### &nbsp; &nbsp; &nbsp; &nbsp; Determine the number of neurons in the hidden layers.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The is no rule of thumb to determine the number of neurons, based on experiment.\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Start with a number of hidden neurons between the **number of input features (n)** and **2–3× n**, then **tune** based on **validation performance**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Too few neurons → underfitting.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Too many neurons → overfitting, slower training, more compute.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Use dropout or L2 regularization if you go large.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Always monitor validation loss to adjust the architecture.<br>\n",
    "<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - n is the number of input features and assuming we're doing classification, regression or fully connecting (dense) layers.\n",
    "\n",
    "| Situation                        | Suggested Hidden Neurons                    |\n",
    "| -------------------------------- | ------------------------------------------- |\n",
    "| **Simple problem**               | $\\text{hidden\\_neurons} \\approx n \\ or \\ \\frac{n}{2}$|\n",
    "| **Moderate complexity**          | $\\text{hidden\\_neurons}\\approx 1.5n \\ \\text{or} \\approx 2n$ |\n",
    "| **High complexity / large data** | $\\text{hidden\\_neurons} \\approx 3n$ or more |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88091a2",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 3. Adding the second hidden layer\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Here we will add a 2nd layer in order to build a deep learning model as suppose to a shallow modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "33c2a87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add a new layer we just copy and past the code above.\n",
    "    # i.e. the 2nd layer is added the first way as the 1st.\n",
    "ann.add(tf.keras.layers.Dense(units = 5 , activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ef969",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 4. Adding the output layer\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Here we will add the output layer which will contain what we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8e5dfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ann.add(tf.keras.layers.Dense(units = 1 , activation = 'sigmoid'))  # We're still using the dense class because we want to create a fully connected layer i.e. ...\n",
    "                                                                    # ... we want our output layer to be connected to our input layer.\n",
    "                                                                    # Since the output i.e. exited in our dataset is binary i.e. can only be 0 or 1, therefore ...\n",
    "                                                                    # ... we need one neuron i.e perceptron in the output layer, this is because with a perceptron ...\n",
    "                                                                    # ... we can only have 0 or 1 as output.                                                              \n",
    "# for the output layer we want to have a activation function is the Sigmoid activation function, this because ...\n",
    "# ... the sigmoid function(only for binary prediction) will not only give us the prediction but also it will give us the probability od the out come.\n",
    "# ... the softmax function(only for categorical prediction) will give us the probability od the out come."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde919f",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 1. Compiling the ANNwith an optimizer, a loss function and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9a46a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the ann object to call the compile() method\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])   # optimizer is the learning algorithm\n",
    "                                                                                            # the best optimizer is the one that can perform stochastic gradient descent...\n",
    "                                                                                            # ... which is the adam optimizer.\n",
    "                                                                                            # SGD will update the weights in order to reduce the loss function\n",
    "                                                                                       # loss is the loss function\n",
    "                                                                                            # binary_crossentropy is the loss function for binary prediction\n",
    "                                                                                            # categorical_crossentropy is the loss function for categorical prediction\n",
    "                                                                                       # metrics is the metric used to evaluate the performance of the model.\n",
    "                                                                                            # metric should be enter in a pair of square bracket since we can enter several...\n",
    "                                                                                            # ... metrics in the same time.\n",
    "                                                                                            # metrics could be: accuracy, precision, recall\n",
    "                                                                                                # metrics = ['accuracy'] is the accuracy metric\n",
    "                                                                                                # metrics = ['accuracy', 'precision'] is the accuracy and precision metric\n",
    "                                                                                                # metrics = ['accuracy', 'precision', 'recall'] is the accuracy, precision and recall metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545345d5",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Training the ANN on the Training set over certain number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f6e841a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7959 - loss: 0.5302\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8076 - loss: 0.4538\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8117 - loss: 0.4353\n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8151 - loss: 0.4273\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8184 - loss: 0.4216\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8217 - loss: 0.4165\n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8253 - loss: 0.4122\n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.4078\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.4041\n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8320 - loss: 0.4010\n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8320 - loss: 0.3976\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8345 - loss: 0.3951\n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8329 - loss: 0.3925\n",
      "Epoch 14/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8338 - loss: 0.3899\n",
      "Epoch 15/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8345 - loss: 0.3872\n",
      "Epoch 16/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8339 - loss: 0.3848\n",
      "Epoch 17/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8339 - loss: 0.3821\n",
      "Epoch 18/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8347 - loss: 0.3796\n",
      "Epoch 19/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8335 - loss: 0.3770\n",
      "Epoch 20/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8353 - loss: 0.3743\n",
      "Epoch 21/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8410 - loss: 0.3719\n",
      "Epoch 22/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8464 - loss: 0.3694\n",
      "Epoch 23/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8476 - loss: 0.3670\n",
      "Epoch 24/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8472 - loss: 0.3650\n",
      "Epoch 25/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8495 - loss: 0.3631\n",
      "Epoch 26/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8499 - loss: 0.3616\n",
      "Epoch 27/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8506 - loss: 0.3602\n",
      "Epoch 28/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8522 - loss: 0.3589\n",
      "Epoch 29/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8524 - loss: 0.3576\n",
      "Epoch 30/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8543 - loss: 0.3565\n",
      "Epoch 31/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8543 - loss: 0.3557\n",
      "Epoch 32/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8543 - loss: 0.3538\n",
      "Epoch 33/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8564 - loss: 0.3531\n",
      "Epoch 34/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8579 - loss: 0.3513\n",
      "Epoch 35/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8576 - loss: 0.3501\n",
      "Epoch 36/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8581 - loss: 0.3490\n",
      "Epoch 37/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8590 - loss: 0.3480\n",
      "Epoch 38/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8618 - loss: 0.3471\n",
      "Epoch 39/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8614 - loss: 0.3453\n",
      "Epoch 40/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8620 - loss: 0.3445\n",
      "Epoch 41/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8621 - loss: 0.3435\n",
      "Epoch 42/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8614 - loss: 0.3437\n",
      "Epoch 43/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8624 - loss: 0.3420\n",
      "Epoch 44/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.3418\n",
      "Epoch 45/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8626 - loss: 0.3403\n",
      "Epoch 46/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8611 - loss: 0.3408\n",
      "Epoch 47/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8615 - loss: 0.3394\n",
      "Epoch 48/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8640 - loss: 0.3400\n",
      "Epoch 49/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.3391\n",
      "Epoch 50/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8644 - loss: 0.3384\n",
      "Epoch 51/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8616 - loss: 0.3386\n",
      "Epoch 52/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8625 - loss: 0.3380\n",
      "Epoch 53/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8639 - loss: 0.3381\n",
      "Epoch 54/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8624 - loss: 0.3376\n",
      "Epoch 55/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8658 - loss: 0.3375\n",
      "Epoch 56/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8644 - loss: 0.3372\n",
      "Epoch 57/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8630 - loss: 0.3377\n",
      "Epoch 58/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3372\n",
      "Epoch 59/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8658 - loss: 0.3372\n",
      "Epoch 60/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8640 - loss: 0.3364\n",
      "Epoch 61/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8639 - loss: 0.3370\n",
      "Epoch 62/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3370\n",
      "Epoch 63/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.3357\n",
      "Epoch 64/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8648 - loss: 0.3363\n",
      "Epoch 65/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8639 - loss: 0.3371\n",
      "Epoch 66/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.3363\n",
      "Epoch 67/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8633 - loss: 0.3362\n",
      "Epoch 68/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8664 - loss: 0.3360\n",
      "Epoch 69/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8619 - loss: 0.3369\n",
      "Epoch 70/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8645 - loss: 0.3361\n",
      "Epoch 71/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8627 - loss: 0.3364\n",
      "Epoch 72/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.3361\n",
      "Epoch 73/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8631 - loss: 0.3360\n",
      "Epoch 74/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3363\n",
      "Epoch 75/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.3352\n",
      "Epoch 76/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8633 - loss: 0.3356\n",
      "Epoch 77/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8631 - loss: 0.3359\n",
      "Epoch 78/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.3358\n",
      "Epoch 79/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.3353\n",
      "Epoch 80/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8633 - loss: 0.3357\n",
      "Epoch 81/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8641 - loss: 0.3365\n",
      "Epoch 82/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8641 - loss: 0.3352\n",
      "Epoch 83/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8624 - loss: 0.3355\n",
      "Epoch 84/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8659 - loss: 0.3355\n",
      "Epoch 85/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3358\n",
      "Epoch 86/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8639 - loss: 0.3355\n",
      "Epoch 87/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8635 - loss: 0.3365\n",
      "Epoch 88/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8635 - loss: 0.3351\n",
      "Epoch 89/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3351\n",
      "Epoch 90/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8634 - loss: 0.3358\n",
      "Epoch 91/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3357\n",
      "Epoch 92/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3355\n",
      "Epoch 93/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8634 - loss: 0.3354\n",
      "Epoch 94/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8640 - loss: 0.3353\n",
      "Epoch 95/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8636 - loss: 0.3353\n",
      "Epoch 96/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.3351\n",
      "Epoch 97/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8633 - loss: 0.3353\n",
      "Epoch 98/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8630 - loss: 0.3354\n",
      "Epoch 99/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.3350\n",
      "Epoch 100/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.3349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x224b7179590>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, Y_train, batch_size = 32, epochs = 100)   # batch_size = 32 is the default value\n",
    "                                                           # epochs = 100 is the default value\n",
    "\n",
    "\n",
    "# To train our model, we use the ann object to call the fit() method\n",
    "    # the method to train what ever ML model is the fit() method and will always take the following parameters:\n",
    "        # X_train: the matrix of features of the training set\n",
    "        # Y_train: the dependent variable of the training set\n",
    "            # When training an ANN, we have to enter 2 more parameters:\n",
    "                # batch_size: because batch_size is the number of samples processed before the model is updated\n",
    "                    # batch learning is efficient and performant when training an ANN on a large dataset, ...\n",
    "                    # ... because it allows the model to update its weights in batches instead of updating them one sample at a time\n",
    "                    # The Batch size usually chosen is 32\n",
    "                # epochs: the number of times the model will be trained on the entire dataset, so as to improve the accuracy of the model over time.\n",
    "                    # The number of epochs usually chosen is 100, but we can choose any number as long is it not small in order to learn properly.\n",
    "\n",
    "\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e00f3",
   "metadata": {},
   "source": [
    "- Accuracy 0.8637 means that out of 100 observations, 86% of them are correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450ddab",
   "metadata": {},
   "source": [
    "# Part 4 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe6641",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 1. Predict the result of a single observation\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  our ANN model to predict the result of a single observation<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Geography**: France<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Credit Score**: 600<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Gender**: Male<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Age**: 40<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Tenure**: 3 <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Balance**: 60000<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Number of Products**: 2<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Has Credit Card**: Yes<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Is Active Member**: Yes<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Estimated Salary**: 50000<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **So, should we say goodbye to that customer?**<br>\n",
    "\n",
    "### &nbsp; &nbsp; Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cf436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "# In order to predict a single observation, we will use the predict() method of the ann \n",
    "    # Any input of the predict method must be a 2D array i.e. in double brackets.\n",
    "    # Since Geography is a categorical feature, we'll have to see its binary value in the cell right above where we created the dummy variable .i.e.\n",
    "        # where we did the one hot encoding. France was encoded as 1.0, 0.0, 0.0 or 1, 0, 0\n",
    "    # Male was label/encoded as 1 and female as 0.\n",
    "    # since the predict method should be called or applied on the observation with the same scale as the one used for the training. ...\n",
    "        # ... since the sc was used to scaled the observations, we've to call it on the transformed and not the fit_transformed, ...\n",
    "        # ... otherwise will lead to information leakage\n",
    "        # on new observation or when we want to predict a new observation we can only applied the transform method.\n",
    "        \n",
    "print(ann.predict(sc.fit_transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "    # predict() method will return the probability of the observation to be 0 or 1.\n",
    "\n",
    "# if we don't want the result in probability form, we have to do the following:\n",
    "# print(ann.predict(sc.fit_transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)  # meaning the final result/probability is greater than 0.5, it will be 1/True\n",
    "                                                                                               # and if the probability is less than 0.5, it will be 0/False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67049315",
   "metadata": {},
   "source": [
    "- The sigmoid activation function at the output layer will give us the probability of the customer leaving the bank.\n",
    "  - If the probability is **greater than 0.5**, the customer **will leave** the bank.\n",
    "    - **So, should we say goodbye to that customer?** False, meaning the customer **will not leave** the bank.\n",
    "  - If the probability is **less than 0.5**, the customer **will not leave** the bank.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a123bc6",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Predicting the Test set results\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Display next to each other the predicted values and the real values. i.e. vector of predicted values and vector of real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dbf19e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "Y_pred = ann.predict(X_test)\n",
    "\n",
    "# Converting the probabilities into binary predictions i.e. 0 or 1\n",
    "Y_pred = (Y_pred > 0.5)           # if the probability is greater than 0.5, it will be 1/True\n",
    "                                  # and if the probability is less than 0.5, it will be 0/False\n",
    "\n",
    "print(np.concatenate((Y_pred.reshape(len(Y_pred),1), Y_test.reshape(len(Y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e4f82",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 3. Making the Confusion Matrix\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Get the final accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "952e60c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1523   72]\n",
      " [ 200  205]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b270c23",
   "metadata": {},
   "source": [
    "- Meaning out of 100 customers, 86.4 were predicted correctly.\n",
    "  - 1523 correct that the customer stay in the bank.\n",
    "  - 205 correct prediction that the customer will leave the bank.\n",
    "  - 200 incorrect prediction that the customer stay in the bank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
