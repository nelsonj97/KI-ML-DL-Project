{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f46da6",
   "metadata": {},
   "source": [
    "# Building ANN using Tensorflow 2 for Regression\n",
    "### (Building an ANN Regression model to predict electrical energy output of a combined cycle power plant)\n",
    "- Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods\n",
    "- The dataset contains 9568 data points collected from a **Combined Cycle Power Plant** over 6 years (2006-2011), when the power plant<br> \n",
    "  was set to work with full load. Features consist **of hourly average ambient variables Temperature (T)**, **Ambient Pressure (AP)**,<br>\n",
    "  **Relative Humidity (RH)** and **Exhaust Vacuum (V)** to predict the **net hourly electrical energy output (EP)  of the plant**.<br>\n",
    "  The data source is [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant).<br>\n",
    "  [Udemy-Regression for this dataset](https://www.udemy.com/course/linear-regression-with-artificial-neural-network/learn/lecture/18889080#overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf9319",
   "metadata": {},
   "source": [
    "# Part 0 -  libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167b06f",
   "metadata": {},
   "source": [
    "### &nbsp; 1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac1a92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d45d1",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed0f0c1",
   "metadata": {},
   "source": [
    "### &nbsp; 1. Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e25041e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset. xlsx file can be read with excel\n",
    "dataset = pd.read_excel('Folds5x2_pp.xlsx')\n",
    "\n",
    "# when we are about to train a model, we always have to create tw0 separate subsets\n",
    "\n",
    "# 1. Creating the Metrics of features i.e all the column containing the features\n",
    "X = dataset.iloc[:, :-1].values     # iloc (index location), to select the value of the following indexes\n",
    "                                    # : means all the rows/columns\n",
    "                                    # :-1 means the except the last column/row\n",
    "                                  \n",
    "# 2. Creating the Dependent variables vector i.e. column/vector containing the dependent variable we want to predict.\n",
    "Y = dataset.iloc[:, -1].values      # -1 means the last column, in order word -1 represent the index of the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5c104e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  14.96   41.76 1024.07   73.17]\n",
      " [  25.18   62.96 1020.04   59.08]\n",
      " [   5.11   39.4  1012.16   92.14]\n",
      " ...\n",
      " [  31.32   74.33 1012.92   36.48]\n",
      " [  24.48   69.45 1013.86   62.39]\n",
      " [  21.6    62.52 1017.23   67.87]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da873764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[463.26 444.37 488.56 ... 429.57 435.74 453.28]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfda3a2",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Encoding categorical data\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  **‼️ We don't have categorical variable ‼️**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceaf599",
   "metadata": {},
   "source": [
    "#### &nbsp; &nbsp; &nbsp; &nbsp; ❌2.1 Label Encoding the \"Gender\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee0ddd",
   "metadata": {},
   "source": [
    "#### &nbsp; &nbsp; &nbsp; &nbsp; ❌2.2 One Hot Encoding the \"name_of_coutries\" column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc825f1",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 3. Splitting the dataset into the Training set and Test set\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - We want to train our ANN on a separate set called **Training set** <br> \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - And evaluate it performance on a separate set called **Training set** <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "03f7a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_selection in scikit learn contain the train_test_split function, that allow us to split our dataset into training and test sets.\n",
    "from sklearn .model_selection import train_test_split\n",
    "\n",
    "# Create 4 variable to collect what will be return by the train_test_split_function\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "# 0.2 means 20% of the dataset will be used for testing\n",
    "# 0.2 means 80% of the dataset will be used for training\n",
    "# random_state means that the split will be the same each time we run the program\n",
    "# test_size means that the test set will be 20% of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe80c4",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 4. Feature Scaling \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; **Note**: Feature Scaling is absolute compulsory in Deep Learning. <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e. We're **normalizing** or **standardizing** the data to have a mean of zero and a standard deviation of one. <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - And will be applied to all our feature variables irrespective of whether they are already in the desire scale/range <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ac05b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Applying feature scaling feature of all the training and test set i.e. on only the feature.\n",
    "X_train = sc.fit_transform(X_train)         # fit_transform is fitted to the train set in order to avoid information leakage.\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb573444",
   "metadata": {},
   "source": [
    "# Part 2 - Building the ANN\n",
    "- Structure of the ANN we're going to build\n",
    "  \n",
    "  ![alt text](image.png)\n",
    "  - We have 4 layers:\n",
    "    - Input Layer in yellow\n",
    "    - First Hidden Layer containing 6 neurons in green    ✅\n",
    "    - Second Hidden Layer containing 6 neurons in green   ✅\n",
    "    - Output Layer containing 1 neuron in red\n",
    "  - We can change the architecture if we want i.e. instead of 6 neurons in each layer we can have any  number of neurons.\n",
    "  \n",
    "  #### &nbsp; &nbsp; &nbsp; &nbsp; Determine the number of neurons in the hidden layers.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The is no rule of thumb to determine the number of neurons, based on experiment.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Start with a number of hidden neurons between the **number of input features (n)** and **2–3× n**, then **tune** based on **validation performance**<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Too few neurons → underfitting.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Too many neurons → overfitting, slower training, more compute.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Use dropout or L2 regularization if you go large.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Always monitor validation loss to adjust the architecture.<br>\n",
    "<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - n is the number of input features and assuming we're doing classification, regression or fully connecting (dense) layers.\n",
    "\n",
    "| Situation                        | Suggested Hidden Neurons                    |\n",
    "| -------------------------------- | ------------------------------------------- |\n",
    "| **Simple problem**               | $\\text{hidden\\_neurons} \\approx n \\ or \\ \\frac{n}{2}$|\n",
    "| **Moderate complexity** ✅       | $\\text{hidden\\_neurons}\\approx 1.5n \\ \\text{or} \\approx 2n$ |\n",
    "| **High complexity / large data** | $\\text{hidden\\_neurons} \\approx 3n$ or more |\n",
    "\n",
    "$$\\text{hidden\\_neurons}\\approx 1.5n = 1.5*4 = 1.5*4 = 6 ✅$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a4a65",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 1. Initializing the ANN\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Here we'll initialize the ANN as a sequence of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a7ee529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an object of the Sequential class \n",
    "   # Sequential class belong to the models module of the keras library\n",
    "        # And the keras library belong to the tensorflow module\n",
    "    \n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cfee3",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "029b03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the add() method we can add layers to the                   \n",
    "ann.add(tf.keras.layers.Dense(units = 6 , activation = 'relu'))    # Dense stand for the connection between the input layer and 1st hidden layer.\n",
    "                                                                        # Meaning each neuron in the input should be full connected to each neuron in the hidden layer.\n",
    "                                                                            # The Dense class is taken from the layers module of the keras library.\n",
    "                                                                            # The layer module contains tools to add layers in our ANN.\n",
    "                                                                   # units corresponds to the number of neurons we want to have in the 1st hidden layer. \n",
    "                                                                   # The number of neurons in the input layer is equal to the number of features variables\n",
    "                                                                        # We don't have to specify the number of neurons in the input layer because Tensorflow will do it automatically for us.\n",
    "                                                    # The activation function is the Rectifier activation function i.e. the RELU activation function\n",
    "                                                    # The RELU activation function is one of the most popular activation functions.\n",
    "# My  unit = number of input features * 1.5 = 4 * 1.5 = 6\n",
    "# 6 is called the number of hidden layers or hyperparameter value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88091a2",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 3. Adding the second hidden layer\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Here we will add a 2nd layer in order to build a deep learning model as suppose to a shallow modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab836ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just copy the above code and past it here in order to add a second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units = 6 , activation = 'relu'))        # units corresponds to the number of neurons we want to have in the 2st hidden layer...  \n",
    "                                                                       # ... which will be connected automatically to the previous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ef969",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 4. Adding the output layer\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Here we will add the output layer which will contain what we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just copy the above code and past it here in order to add a second hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units = 1))       # units corresponds to the number of neurons we want to have at the output layer...  \n",
    "                                                    # ... which will be connected automatically to the previous.\n",
    "                                                # For the activation function we can use sigmoid or softmax or no activation function....\n",
    "                                                    #... but the sigmoid(for only 2 categories to predict in the end i.e. 0 or 1) and ...\n",
    "                                                    #... softmax(for more tha 2 categories to predict in the end) are used for classification problems and here we're...\n",
    "                                                    #... dealing with regression(i.e. when we want to predict continuous number as output) problems, therefore we should use no activation function.       \n",
    "                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efd4fe",
   "metadata": {},
   "source": [
    "# Part 3 - Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde919f",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 1. Compiling the ANN with an optimizer, a loss function and metrics\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - **Optimizer**: is the algorithm with which we'll perform Stochastic Gradient Descent <br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; i.e. updating the weights through network using backpropagation in order to reduce the loss<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Loss function is the function that we'll use to measure the performance of our model.<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Metrics is the metric that we'll use to evaluate the performance of our model.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85e03a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the compile method of the Sequential class to compile the model\n",
    "ann.compile(optimizer = 'adam', loss = 'mean_squared_error')   # the optimizer to perform SGD is the adam optimizer, in order to reduce the loss\n",
    "                                # The loss function to choose for regression problem is the mean_squared_error/rmse(root mean squared error) loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545345d5",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Training the ANN on the Training set over certain number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d21390aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 204200.3906\n",
      "Epoch 2/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 190275.5156\n",
      "Epoch 3/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 148726.7656\n",
      "Epoch 4/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 87974.5859 \n",
      "Epoch 5/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41592.6992\n",
      "Epoch 6/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23195.7773\n",
      "Epoch 7/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17376.7480\n",
      "Epoch 8/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13861.2676\n",
      "Epoch 9/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10859.5537\n",
      "Epoch 10/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8265.3906\n",
      "Epoch 11/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6075.4277\n",
      "Epoch 12/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4299.8286\n",
      "Epoch 13/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2945.2725\n",
      "Epoch 14/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1970.6490\n",
      "Epoch 15/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1310.3540\n",
      "Epoch 16/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 883.4843\n",
      "Epoch 17/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 611.3537\n",
      "Epoch 18/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 436.5932\n",
      "Epoch 19/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 318.4195\n",
      "Epoch 20/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 235.8656\n",
      "Epoch 21/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 177.5192\n",
      "Epoch 22/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 135.8392\n",
      "Epoch 23/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 105.6911\n",
      "Epoch 24/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 83.9465\n",
      "Epoch 25/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.3494\n",
      "Epoch 26/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 56.8717\n",
      "Epoch 27/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 48.6511\n",
      "Epoch 28/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 42.2935\n",
      "Epoch 29/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37.5691\n",
      "Epoch 30/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33.7665\n",
      "Epoch 31/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 31.0877\n",
      "Epoch 32/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 28.9068\n",
      "Epoch 33/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 27.2192\n",
      "Epoch 34/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.9244\n",
      "Epoch 35/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24.8659\n",
      "Epoch 36/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 24.1403\n",
      "Epoch 37/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 23.4951\n",
      "Epoch 38/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 23.0462\n",
      "Epoch 39/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 22.6688\n",
      "Epoch 40/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 22.3281\n",
      "Epoch 41/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 22.0205\n",
      "Epoch 42/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.8839\n",
      "Epoch 43/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.8870\n",
      "Epoch 44/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.6545\n",
      "Epoch 45/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.6460\n",
      "Epoch 46/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.5594\n",
      "Epoch 47/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.5541\n",
      "Epoch 48/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.4915\n",
      "Epoch 49/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.6091\n",
      "Epoch 50/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.5942\n",
      "Epoch 51/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.3970\n",
      "Epoch 52/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.3639\n",
      "Epoch 53/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.3853\n",
      "Epoch 54/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.3000\n",
      "Epoch 55/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2996\n",
      "Epoch 56/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.4457\n",
      "Epoch 57/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.4255\n",
      "Epoch 58/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2925\n",
      "Epoch 59/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.4146\n",
      "Epoch 60/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.4588\n",
      "Epoch 61/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.3146\n",
      "Epoch 62/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2615\n",
      "Epoch 63/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2898\n",
      "Epoch 64/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.4276\n",
      "Epoch 65/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.4066\n",
      "Epoch 66/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.2941\n",
      "Epoch 67/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.3367\n",
      "Epoch 68/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.3110\n",
      "Epoch 69/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2665\n",
      "Epoch 70/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.3964\n",
      "Epoch 71/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2947\n",
      "Epoch 72/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2164\n",
      "Epoch 73/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.4059\n",
      "Epoch 74/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.3500\n",
      "Epoch 75/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.3183\n",
      "Epoch 76/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.3120\n",
      "Epoch 77/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.1969\n",
      "Epoch 78/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.5179\n",
      "Epoch 79/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2451\n",
      "Epoch 80/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2453\n",
      "Epoch 81/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2087\n",
      "Epoch 82/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.1742\n",
      "Epoch 83/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.4065\n",
      "Epoch 84/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.3045\n",
      "Epoch 85/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2421\n",
      "Epoch 86/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 21.1579\n",
      "Epoch 87/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2580\n",
      "Epoch 88/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2736\n",
      "Epoch 89/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2652\n",
      "Epoch 90/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.1963\n",
      "Epoch 91/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2715\n",
      "Epoch 92/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2293\n",
      "Epoch 93/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.1913\n",
      "Epoch 94/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2984\n",
      "Epoch 95/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2334\n",
      "Epoch 96/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.2693\n",
      "Epoch 97/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 21.2098\n",
      "Epoch 98/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.3015\n",
      "Epoch 99/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.2067\n",
      "Epoch 100/100\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.3274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x298bf5182d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the fit() method to train our model\n",
    "ann.fit(X_train, Y_train, batch_size = 32, epochs = 100 )   # Here we've to specify on which set we want to fit our ann (i.e. ANN) i.e. X_train and Y_train\n",
    "                                                            # Here we've to specify the batch_size and the number of epochs\n",
    "                                                                # over each epochs/rounds the loss is slightly reduced.\n",
    "                                                                # epochs = 100 is the default value\n",
    "                                                            # batch_size is the number of feature which we want to forwardpropagate at a time\n",
    "                                                                # batch_size = 32 is the default value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7957c",
   "metadata": {},
   "source": [
    "- We can see that the loss get reduce over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450ddab",
   "metadata": {},
   "source": [
    "# Part 4 - Making the predictions and evaluating the \n",
    "- Since our Neural brain is already smart we can use it to predict the result of new observation\n",
    "- Now we can start with **inference**, which consist of predicting the result of new observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe6641",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 1. Predict the result of a single observation\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  - We use our ANN model to predict the result of a single observation<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; \n",
    "\n",
    "### &nbsp; &nbsp; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a123bc6",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; 2. Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b494e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Using the predict() method to predict the result of the test set\n",
    "    # Y_pred wil store all the prediction of the test set\n",
    "Y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f202",
   "metadata": {},
   "source": [
    "- Displaying next to each other the predicted values and the real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37227503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[430.76 431.23]\n",
      " [459.1  460.01]\n",
      " [463.77 461.14]\n",
      " ...\n",
      " [470.31 473.26]\n",
      " [442.41 438.  ]\n",
      " [462.5  463.28]]\n"
     ]
    }
   ],
   "source": [
    "# Comparing the predicted and real result\n",
    "np.set_printoptions(precision = 2)                                  # setting the result to 2 decimal places\n",
    "\n",
    "# Since the dependent variable as we can see in part 2 is horizontal we have to reshape it vertically, so that we can easily compare the predicted and real result...\n",
    "# ... then concatenate the 2 vectors next to each other and then print the result\n",
    "print(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))       # reshape is used to reshape the vector vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80849238",
   "metadata": {},
   "source": [
    "#### The prediction is each time very close to our real result, which Awesome, meaning that our ANN performed very well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e4f82",
   "metadata": {},
   "source": [
    "### &nbsp; &nbsp; ❌3. Making the Confusion Matrix   <== ‼️ (not use in regression, only for classification)‼️\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Get the final accuracy on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
