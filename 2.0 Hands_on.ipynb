{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cf5fd4",
   "metadata": {},
   "source": [
    "# Hands-on Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf75f69",
   "metadata": {},
   "source": [
    "# 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "27fabd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf4f8f",
   "metadata": {},
   "source": [
    "# 1. Importing The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0c8528a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create a variable to store the dataset\n",
    "data_set = pd.read_csv('Data.csv')\n",
    "\n",
    "# 2. Create 2 new Entities(1 for the Metrics of feature(usually the first column) and 2nd for the dependent variable vector(usually the last column))\n",
    "# The Features(country, Age, Salary): is the column with which we're going to predict the dependent variable(Purchase)\n",
    "#    : mean all row, all column/feature except -1(-1 mean except index of the last column).\n",
    "X = data_set.iloc[:, :-1].values     # iloc: stand for loc index, will take the index we want to extract form dataset.\n",
    "                                     # iloc: without upper and lower band ie [:] mean we're interested in all the range/row/column\n",
    "                                     # values mean we  are taking the value of the specify columns/features.\n",
    "\n",
    "# The dependent Variable(i.e Purchase): is what we want to predict and is most f the time th last column.\n",
    "#        all the row, -1 mean last column\n",
    "Y = data_set.iloc[:, -1 ].values     # ioc: used to collect/extract the index of the row and column that we want.\n",
    "\n",
    "# Our future ML-Model expect exactly X and Y in their input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a750246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d704586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a90e6",
   "metadata": {},
   "source": [
    "# 2. Handling Missing Data\n",
    "Missing Data may cause some error when training the ML Model, therefor must be handle. <br>\n",
    "- They are several ways to handle missing Data <br>\n",
    "  - 1. By deleting it, especially when we have many data in our data points in our data set<br>\n",
    "  - 2. Replacing the missing value by the average of all the values of the column in which the data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "527a10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Replacing the missing value by the average of all the values of the column in which the data is missing.\n",
    "# We are going to use sklearn to handle missing data, to do this we have to import the SimpleImputer class.\n",
    "    # then create an object of SimpleImputer class which will allow us to store the mean/average\n",
    "from sklearn.impute import SimpleImputer      # the SimpleImputer  class belong to the module impute\n",
    "                                              # dot is used to access the module\n",
    "\n",
    "# Creating an instance of the class i.e create an object to store the average\n",
    "                        # first argument is to specify which value we want to replace.\n",
    "                                                 # 2nd Argument is to specify with what we want to replace the missing value, here wih the mean\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy='mean' )\n",
    "\n",
    "# 2. Connecting the imputer to the features using the fit() method\n",
    "# apply the imputer.fit()) only on numerical column.\n",
    "imputer.fit(x[:, 1:3])  # Here we connect/fit imputer to the missing data from column 1 to (2 + 1)=3, +1 since the upperbound is excluded in python\n",
    "                        # fit() will look for all the missing value in the numerical salary and age column\n",
    "# fit() will also calculate the average\n",
    "# transform() method will then replace the missing salary through the average and then return the new updated version of the matrix of feature.\n",
    "    # the new updated version is then store in the matrix of feature.\n",
    "X[:, 1:3] =imputer.transform(x[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c4b9125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the matrix of feature to check if the update was successful.\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28251555",
   "metadata": {},
   "source": [
    "# 3. Encoding Categorical data.\n",
    "As we can see the data set contains one column with categories, i.e the first column\n",
    "- Idea:\n",
    "  - Here we perform One-Hot encoding\n",
    "  - A popular method used to preprocess the data containing categorical variable.\n",
    "    - One-Hot encoding consist of turn the country column/category into 3 separate 3 (i.e France, Spain, Germany)<br>\n",
    "      because the are 3 (i.e France, Spain, Germany) different classes in the country category.\n",
    "    - One-Hot encoding will create binary vector for each of the class/country.\n",
    "      - France will for instance have the vector 100, spain 010, and Germany 001. So that the will not exist a numeric other <br>\n",
    "        between  the two country.\n",
    "        - i.e the country column will be replace by 3 new column containing their respective vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "046629b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We need 2 classes i.e column-transform class from compose sklearn library and One-Hot encoder from the same sklearn library.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 2. Create an object of the column-Transform class.\n",
    "                # We have to give two arguments. 1. Transformer and 2. remainder\n",
    "                # 1. Transformer: here we specify what kind of transformation and encoding we want to do and on which index of the column.\n",
    "                # 2.remainder: here we specify the column which, we don't want to apply some transformation i.e. Age and Salary\n",
    "                # must be done in a pair of square bracket and parenthesis.\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0] )], # encoder is the type of transformation,  OneHotEncoder type of encoder we want, apply on column 0\n",
    "                       remainder= 'passthrough')     # passthrough mean we don't want to apply One-hot encoder to the remainder column(Age and Salary)\n",
    "\n",
    "# Now we have to connect our ct object to our matrix of feature X\n",
    "# x = ct.fit_transform(x)     # with fit_transform() it's going to fit into our x and transform our matrix X and then the result will be store in x\n",
    "                              # fit_transform() does not return X as a numpy array, since X will expected later during training as a numpy array ...\n",
    "                              # .... we have to convert X now as a numpy array\n",
    "X = np.array(ct.fit_transform(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ee8b038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Checking the result by printing X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e5929",
   "metadata": {},
   "source": [
    "As we can see France has been encoded as 100, \n",
    "France  as 100\n",
    "spain   as 001\n",
    "Germany as 010\n",
    "\n",
    "That is each country has been assigned a unique ID, that is our country has been turn into a numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128b6f5",
   "metadata": {},
   "source": [
    "# 4. Encoding the Independent Variables\n",
    "In order to do that we are going to use another class call **label encoder**, <br>\n",
    "which will convert the trend into zero and 0ne respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "08c77e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 1. we are going to use the label encoder class from the sklearn library called preprocessing.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 2. we then create an object of the class\n",
    "le = LabelEncoder()    # we don't need to input any thing in the parenthesis because its ..\n",
    "                       # just one single vector that will be need to encode i.e. either zero or one\n",
    "\n",
    "Y = le.fit_transform(Y)    # will fit/connect to Y and then convert/transform Y into either null or zero, the updated result \n",
    "                           # ... will be return to Y\n",
    "\n",
    "# Checking our result.\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197199ff",
   "metadata": {},
   "source": [
    "# 4. Splitting\n",
    "- Each time we want to train a machine learning model, we have to split our data into\n",
    "  - Training set: used to train the ML model on existing observation in order to understand the correlation inside data set.\n",
    "    - When the ML-model is train so well on the training set, so well that it doesn't perform well on  new observation.\n",
    "  - Test set: used to evaluate the ML-modeL on new observation, in order to check that the is not over fitting on new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a8be70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sklearn contain a module which itself contain the train, test and split method.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 2. This method will create 4 separate set, because we will actually create a pair of matrix of feature and dependent variable for...\n",
    "# ... the training set and another pair of matrix of feature an independent variable for the test set.\n",
    "# i.e. X_train which is the matrix and feature of the training set\n",
    "#      X_test which the matrix and feature of the test set.\n",
    "#      Y_train which is the dependent variable of the training set\n",
    "#      Y_test  which is the dependent variable of the test set.\n",
    "X_train, X_test , Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 1 )  #  expect data of feature, independent variable as parameter, split size...\n",
    "                                                                                              #  80% of the data in the training and 20% in the test\n",
    "                                                                                              #  random_state = 1 for teaching purpose, so ..\n",
    "                                                                                              # ... that we all have the same random factor/split.\n",
    "\n",
    "# Observing the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "077f1f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c417c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "84bd2a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a2a7be20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c62040",
   "metadata": {},
   "source": [
    "# 5. Feature scaling\n",
    "**Note**: Feature scaling should always be applied after splitting the data into training and test set.<br> \n",
    "- Feature scaling: consist of putting our variables all on the same scale.\n",
    "- Feature scaling: consist of scaling our variable/feature to make sure they all take the value on the same scale, so as <br>\n",
    "  to prevent one feature/value to dominant the order.\n",
    "- We are not suppose to work with the train set for the training.\n",
    "- Feature scale is a technique to get the mean and standard deviation of our features.\n",
    "- If feature Scaling is done before the splitting then it will actually get mean and standard deviation of all the values<br>\n",
    "  including the one in the test set and this will cause some information leakage, which we are not suppose to get, only on <br>\n",
    "  on new data and new observation.\n",
    "  - Answer: is to prevent information leakage from the test set, which we are not suppose to have until the training is done.<br>\n",
    "\n",
    "**Note**: Feature scaling is not applicable for all ML-Model.\n",
    "- Two most common techniques are \n",
    "  - Normalization: only work for specific cases.\n",
    "  - Standardization: it work all the time and only apply after the split on both test set and train set separately\n",
    "\n",
    "\n",
    "- Feature scaling will be applied to both X_train and X_test separately and scaler will be fitted to only X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "508499ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import the class standard scaler, which will perform standardization on both metric of feature of the training and test sets.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2. Create an object of the class\n",
    "sc = StandardScaler()                     # does not take parameter\n",
    "\n",
    "# 3. fit/connect the standardization(sc) to the training set and only on the numeric i.e salary and Age\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])     #  3: mean from 3 and above\n",
    "                                                      # fit will calculate the mean and the standard deviation of each feature\n",
    "                                                      # transform: will apply the calculated std and mean to the metric.\n",
    "\n",
    "#  4. Applying the same scaler of the training set on to the test set, so that we can get the same transformation\n",
    "# only apply the transform method on the test set, because the feature of test set need to scale by the same scaler that was..\n",
    "# ... use on the training set.\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "da443b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)         # Our age and Variable was transformed so that they take new value in the range -3 and +3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7ea52840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)         # the age and salary were also scale so that they take value between -3 and +3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f5d1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# #################### Done ################## #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f398b50",
   "metadata": {},
   "source": [
    "# ===> Building ML Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
